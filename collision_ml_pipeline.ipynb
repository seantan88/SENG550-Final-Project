{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e199d11a",
   "metadata": {},
   "source": [
    "## Logisitic Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffdb97",
   "metadata": {},
   "source": [
    "### IMPORTS, INITIALIZE SPARK SESSION AND LOAD CLEANED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:13:51.723112Z",
     "start_time": "2024-12-16T07:13:27.797548Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 12:40:17 WARN Utils: Your hostname, Seans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.158 instead (on interface en0)\n",
      "24/12/17 12:40:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/17 12:40:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+---------------+---------------+--------+---------+------------+\n",
      "|           CASE_ID|ACCIDENT_YEAR|PROC_DATE|JURIS|COLLISION_DATE|COLLISION_TIME|OFFICER_ID|REPORTING_DISTRICT|DAY_OF_WEEK|POPULATION|CNTY_CITY_LOC|BEAT_TYPE|CHP_BEAT_TYPE|     PRIMARY_RD|   SECONDARY_RD|DISTANCE|DIRECTION|INTERSECTION|\n",
      "+------------------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+---------------+---------------+--------+---------+------------+\n",
      "|100010101011401155|         2001| 20010416| 0100|      20010101|           114|      1155|                 0|          1|         4|          198|        0|            0|      DUBLIN BL|    SCARLETT CT|     267|        W|           N|\n",
      "|100010103174503131|         2001| 20010416| 0100|      20010103|          1745|      3131|                10|          3|         4|          198|        0|            0|   DOUGHERTY RD|  AMADOR VLY BL|      80|        N|           N|\n",
      "|100010104134002415|         2001| 20010608| 0100|      20010104|          1340|      2415|                 0|          4|         4|          198|        0|            0|   DOUGHERTY RD| RT 580 WBOFF/R|       0|        -|           Y|\n",
      "|100010104170500911|         2001| 20010416| 0100|      20010104|          1705|       911|                 0|          4|         4|          198|        0|            0|   SAN RAMON RD|      DUBLIN BL|      20|        S|           N|\n",
      "|100010104184400911|         2001| 20010416| 0100|      20010104|          1844|       911|                 0|          4|         4|          198|        0|            0|FREDERICKSON LN|FREDERICKSON CT|       0|        -|           Y|\n",
      "+------------------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+---------------+---------------+--------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|CASE_ID|PARTY_NUMBER|VICTIM_ROLE|VICTIM_SEX|VICTIM_AGE|VICTIM_DEGREE_OF_INJURY|VICTIM_SEATING_POSITION|VICTIM_SAFETY_EQUIP1|VICTIM_SAFETY_EQUIP2|VICTIM_EJECTED|\n",
      "+-------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|2912524|           3|          2|         F|        29|                      4|                      3|                   L|                   G|             0|\n",
      "|2912529|           1|          2|         M|        16|                      0|                      3|                   L|                   G|             0|\n",
      "|2912529|           1|          2|         F|        18|                      4|                      4|                   M|                   H|             0|\n",
      "|2912530|           1|          2|         M|        24|                      0|                      3|                   M|                   G|             0|\n",
      "|2912530|           1|          2|         M|        26|                      0|                      6|                   P|                   G|             0|\n",
      "+-------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Collision_Victim_Analysis\")\n",
    "         .config(\"spark.executor.memory\", \"4g\")  # Increase executor memory\n",
    "         .config(\"spark.executor.cores\", \"2\")  # Number of cores per executor\n",
    "         .config(\"spark.driver.memory\", \"4g\")  # Increase driver memory\n",
    "         .getOrCreate())\n",
    "# Load cleaned collision data\n",
    "collision_df = spark.read.csv(\"clean_collision_records.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load cleaned victim data\n",
    "victim_df = spark.read.csv(\"clean_victim_records.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Verify loaded data\n",
    "collision_df.show(5)\n",
    "victim_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159b846",
   "metadata": {},
   "source": [
    "### Join the two cleaned datasets (collision record and victim records), create binary target variable for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4d511e33aa92e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:15:03.039263Z",
     "start_time": "2024-12-16T07:14:03.061881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 12:40:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|CASE_ID|ACCIDENT_YEAR|PROC_DATE|JURIS|COLLISION_DATE|COLLISION_TIME|OFFICER_ID|REPORTING_DISTRICT|DAY_OF_WEEK|POPULATION|CNTY_CITY_LOC|BEAT_TYPE|CHP_BEAT_TYPE|PRIMARY_RD|SECONDARY_RD|DISTANCE|DIRECTION|INTERSECTION|PARTY_NUMBER|VICTIM_ROLE|VICTIM_SEX|VICTIM_AGE|VICTIM_DEGREE_OF_INJURY|VICTIM_SEATING_POSITION|VICTIM_SAFETY_EQUIP1|VICTIM_SAFETY_EQUIP2|VICTIM_EJECTED|\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           1|          2|         M|        17|                      0|                      3|                   C|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           1|          2|         M|        17|                      0|                      6|                   C|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           2|          1|         M|        16|                      4|                      1|                   G|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           2|          2|         M|        16|                      4|                      3|                   G|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           1|          2|         M|        17|                      0|                      3|                   C|                   -|             0|\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after join: 18710630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Join the datasets on CASE_ID\n",
    "combined_df = collision_df.join(victim_df, \"CASE_ID\", \"inner\")\n",
    "\n",
    "\n",
    "# Show the joined data\n",
    "combined_df.show(5)\n",
    "print(\"Total records after join:\", combined_df.count())\n",
    "\n",
    "# change victim degree of injury to binary values\n",
    "combined_df = combined_df.withColumn(\"INJURY_SEVERITY_BINARY\", when((combined_df[\"VICTIM_DEGREE_OF_INJURY\"] == 1) | (combined_df[\"VICTIM_DEGREE_OF_INJURY\"] == 2), 1).otherwise(0))\n",
    "\n",
    "# drop reporting district column - unused\n",
    "cols_to_drop = ['REPORTING_DISTRICT']\n",
    "combined_df = combined_df.drop(*cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05fcba",
   "metadata": {},
   "source": [
    "### Encode categorical columns for use in logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c2053df489a9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:12.597643Z",
     "start_time": "2024-12-16T07:22:39.047671Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------+-----+--------------+--------------+----------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+----------------------+-----------------------------+-----------------------------+--------------------------+--------------------------+--------------------+---------------+------------------+-----------------+---------------------------+---------------------------+------------------------+------------------------+------------------+-------------+----------------+---------------+\n",
      "|CASE_ID|ACCIDENT_YEAR|PROC_DATE|JURIS|COLLISION_DATE|COLLISION_TIME|OFFICER_ID|DAY_OF_WEEK|POPULATION|CNTY_CITY_LOC|BEAT_TYPE|CHP_BEAT_TYPE|PRIMARY_RD|SECONDARY_RD|DISTANCE|DIRECTION|INTERSECTION|PARTY_NUMBER|VICTIM_ROLE|VICTIM_SEX|VICTIM_AGE|VICTIM_DEGREE_OF_INJURY|VICTIM_SEATING_POSITION|VICTIM_SAFETY_EQUIP1|VICTIM_SAFETY_EQUIP2|VICTIM_EJECTED|INJURY_SEVERITY_BINARY|VICTIM_DEGREE_OF_INJURY_index|VICTIM_SEATING_POSITION_index|VICTIM_SAFETY_EQUIP1_index|VICTIM_SAFETY_EQUIP2_index|VICTIM_EJECTED_index|DIRECTION_index|INTERSECTION_index|DAY_OF_WEEK_index|VICTIM_DEGREE_OF_INJURY_vec|VICTIM_SEATING_POSITION_vec|VICTIM_SAFETY_EQUIP1_vec|VICTIM_SAFETY_EQUIP2_vec|VICTIM_EJECTED_vec|DIRECTION_vec|INTERSECTION_vec|DAY_OF_WEEK_vec|\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+----------------------+-----------------------------+-----------------------------+--------------------------+--------------------------+--------------------+---------------+------------------+-----------------+---------------------------+---------------------------+------------------------+------------------------+------------------+-------------+----------------+---------------+\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |1           |2          |M         |17        |0                      |3                      |C                   |-                   |0             |0                     |1.0                          |1.0                          |11.0                      |1.0                       |0.0                 |1.0            |0.0               |2.0              |(7,[1],[1.0])              |(18,[1],[1.0])             |(28,[11],[1.0])         |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |(6,[2],[1.0])  |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |1           |2          |M         |17        |0                      |6                      |C                   |-                   |0             |0                     |1.0                          |2.0                          |11.0                      |1.0                       |0.0                 |1.0            |0.0               |2.0              |(7,[1],[1.0])              |(18,[2],[1.0])             |(28,[11],[1.0])         |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |(6,[2],[1.0])  |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |2           |1          |M         |16        |4                      |1                      |G                   |-                   |0             |0                     |0.0                          |0.0                          |1.0                       |1.0                       |0.0                 |1.0            |0.0               |2.0              |(7,[0],[1.0])              |(18,[0],[1.0])             |(28,[1],[1.0])          |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |(6,[2],[1.0])  |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |2           |2          |M         |16        |4                      |3                      |G                   |-                   |0             |0                     |0.0                          |1.0                          |1.0                       |1.0                       |0.0                 |1.0            |0.0               |2.0              |(7,[0],[1.0])              |(18,[1],[1.0])             |(28,[1],[1.0])          |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |(6,[2],[1.0])  |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |1           |2          |M         |17        |0                      |3                      |C                   |-                   |0             |0                     |1.0                          |1.0                          |11.0                      |1.0                       |0.0                 |1.0            |0.0               |2.0              |(7,[1],[1.0])              |(18,[1],[1.0])             |(28,[11],[1.0])         |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |(6,[2],[1.0])  |\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+----------------------+-----------------------------+-----------------------------+--------------------------+--------------------------+--------------------+---------------+------------------+-----------------+---------------------------+---------------------------+------------------------+------------------------+------------------+-------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Categorical columns to encode\n",
    "encoded_columns = [\"VICTIM_DEGREE_OF_INJURY\",\"VICTIM_SEATING_POSITION\",\"VICTIM_SAFETY_EQUIP1\",\"VICTIM_SAFETY_EQUIP2\",\"VICTIM_EJECTED\", \"DIRECTION\", \"INTERSECTION\", \"DAY_OF_WEEK\"]\n",
    "\n",
    "# Index and encode categorical columns\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid='skip') for c in encoded_columns]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_vec\") for c in encoded_columns]\n",
    "\n",
    "# Assemble features into a single column\n",
    "#feature_columns = [\"DAY_OF_WEEK\", \"COLLISION_TIME\", \"VICTIM_AGE\", \"VICTIM_SEX\", \"DIRECTION_vec\"]\n",
    "\n",
    "# Create the assembler\n",
    "#assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Combine all transformations into a Pipeline\n",
    "encoded_pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Transform the data\n",
    "encoded_df = encoded_pipeline.fit(combined_df).transform(combined_df)\n",
    "\n",
    "encoded_df.show(5, truncate=False)\n",
    "\n",
    "\n",
    "# Select final features and target\n",
    "#final_df = prepared_df.select(\"features\", \"VICTIM_DEGREE_OF_INJURY_index\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "#final_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020a8ea",
   "metadata": {},
   "source": [
    "### Final check for null rows, drop them if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ba13ca6b0879a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:21:23.616593Z",
     "start_time": "2024-12-16T07:21:23.613943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Combined DataFrame:\n",
      "['CASE_ID', 'ACCIDENT_YEAR', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'DAY_OF_WEEK', 'POPULATION', 'CNTY_CITY_LOC', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'PARTY_NUMBER', 'VICTIM_ROLE', 'VICTIM_SEX', 'VICTIM_AGE', 'VICTIM_DEGREE_OF_INJURY', 'VICTIM_SEATING_POSITION', 'VICTIM_SAFETY_EQUIP1', 'VICTIM_SAFETY_EQUIP2', 'VICTIM_EJECTED', 'INJURY_SEVERITY_BINARY']\n",
      "Columns in Encoded DataFrame:\n",
      "['CASE_ID', 'ACCIDENT_YEAR', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'DAY_OF_WEEK', 'POPULATION', 'CNTY_CITY_LOC', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'PARTY_NUMBER', 'VICTIM_ROLE', 'VICTIM_SEX', 'VICTIM_AGE', 'VICTIM_DEGREE_OF_INJURY', 'VICTIM_SEATING_POSITION', 'VICTIM_SAFETY_EQUIP1', 'VICTIM_SAFETY_EQUIP2', 'VICTIM_EJECTED', 'INJURY_SEVERITY_BINARY', 'VICTIM_DEGREE_OF_INJURY_index', 'VICTIM_SEATING_POSITION_index', 'VICTIM_SAFETY_EQUIP1_index', 'VICTIM_SAFETY_EQUIP2_index', 'VICTIM_EJECTED_index', 'DIRECTION_index', 'INTERSECTION_index', 'DAY_OF_WEEK_index', 'VICTIM_DEGREE_OF_INJURY_vec', 'VICTIM_SEATING_POSITION_vec', 'VICTIM_SAFETY_EQUIP1_vec', 'VICTIM_SAFETY_EQUIP2_vec', 'VICTIM_EJECTED_vec', 'DIRECTION_vec', 'INTERSECTION_vec', 'DAY_OF_WEEK_vec']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in Encoded DataFrame: 18710617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3905:===================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in Encoded DataFrame: 18710613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Columns in Combined DataFrame:\")\n",
    "print(combined_df.columns)\n",
    "\n",
    "print(\"Columns in Encoded DataFrame:\")\n",
    "print(encoded_df.columns)\n",
    "\n",
    "print(\"number of records in Encoded DataFrame:\", encoded_df.count())\n",
    "encoded_df = encoded_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "print(\"number of records in Encoded DataFrame:\", encoded_df.count())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf2f85",
   "metadata": {},
   "source": [
    "### Logisitic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6ba41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for Class 1 (Minority): 15.999142350933239\n",
      "Weight for Class 0 (Majority): 0.5161299247622528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+------------------+\n",
      "|            features|INJURY_SEVERITY_BINARY|            weight|\n",
      "+--------------------+----------------------+------------------+\n",
      "|(96,[1,29,47,71,8...|                     0|0.5161299247622528|\n",
      "|(96,[2,29,47,71,8...|                     0|0.5161299247622528|\n",
      "|(96,[0,19,47,71,8...|                     0|0.5161299247622528|\n",
      "|(96,[1,19,47,71,8...|                     0|0.5161299247622528|\n",
      "|(96,[1,29,47,71,8...|                     0|0.5161299247622528|\n",
      "+--------------------+----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+------------------+--------------------+--------------------+----------+\n",
      "|            features|INJURY_SEVERITY_BINARY|            weight|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------------------+------------------+--------------------+--------------------+----------+\n",
      "|(96,[0,18,46,71,8...|                   0.0|0.5161299247622528|[1.47936518934381...|[0.81447667716005...|       0.0|\n",
      "|(96,[0,18,46,71,8...|                   0.0|0.5161299247622528|[1.47936518934381...|[0.81447667716005...|       0.0|\n",
      "|(96,[0,18,46,71,8...|                   0.0|0.5161299247622528|[1.47936518934381...|[0.81447667716005...|       0.0|\n",
      "|(96,[0,18,46,71,8...|                   0.0|0.5161299247622528|[1.47936518934381...|[0.81447667716005...|       0.0|\n",
      "|(96,[0,18,46,71,8...|                   0.0|0.5161299247622528|[1.47936518934381...|[0.81447667716005...|       0.0|\n",
      "+--------------------+----------------------+------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4095:===================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+------------------+-------------------+\n",
      "|summary|INJURY_SEVERITY_BINARY|            weight|         prediction|\n",
      "+-------+----------------------+------------------+-------------------+\n",
      "|  count|              13100117|          13100117|           13100117|\n",
      "|   mean|   0.03125804143581313|1.0000985687211337| 0.2733616806628521|\n",
      "| stddev|   0.17401430571339424|2.6942656576919903|0.44568496426386445|\n",
      "|    min|                   0.0|0.5161299247622528|                0.0|\n",
      "|    max|                   1.0|15.999142350933239|                1.0|\n",
      "+-------+----------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Create a Logistic Regression model to predict injury severity (1 is fatal/severe, 0 is non-fatal/non-severe), and assemble the features into a single column\n",
    "logistic_assembler = VectorAssembler(inputCols=[\"VICTIM_SEATING_POSITION_vec\",\"VICTIM_SAFETY_EQUIP1_vec\" ,\"VICTIM_SAFETY_EQUIP2_vec\", \"VICTIM_EJECTED_vec\", \"DAY_OF_WEEK_vec\", \"INTERSECTION_vec\", \"DIRECTION_vec\"], outputCol=\"features\")\n",
    "log_output = logistic_assembler.transform(encoded_df)\n",
    "total_count = log_output.count()\n",
    "\n",
    "# Calculate class weights\n",
    "class_1_count = log_output.filter(col(\"INJURY_SEVERITY_BINARY\") == 1).count()\n",
    "class_0_count = log_output.filter(col(\"INJURY_SEVERITY_BINARY\") == 0).count()\n",
    "weight_for_class_1 = total_count / (2 * class_1_count)\n",
    "weight_for_class_0 = total_count / (2 * class_0_count)\n",
    "\n",
    "print(\"Weight for Class 1 (Minority):\", weight_for_class_1)\n",
    "print(\"Weight for Class 0 (Majority):\", weight_for_class_0)\n",
    "\n",
    "# Add the class weights to the DataFrame\n",
    "log_output = log_output.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"INJURY_SEVERITY_BINARY\") == 1, lit(weight_for_class_1))\n",
    "    .otherwise(lit(weight_for_class_0))\n",
    ")\n",
    "\n",
    "log_reg1_data = log_output.select(\"features\", \"INJURY_SEVERITY_BINARY\", \"weight\")\n",
    "log_reg1_data.show(5)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = log_reg1_data.randomSplit([0.7, 0.3], seed=7122)\n",
    "\n",
    "\n",
    "\n",
    "# fit the model\n",
    "log_reg1 = LogisticRegression(labelCol=\"INJURY_SEVERITY_BINARY\", featuresCol=\"features\", weightCol=\"weight\")\n",
    "log_reg1_model = log_reg1.fit(train_data)\n",
    "\n",
    "# print summaries\n",
    "log_reg1_summary = log_reg1_model.summary\n",
    "log_reg1_summary.predictions.show(5)\n",
    "log_reg1_summary.predictions.describe().show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49b71d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4104:===================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC:  0.7854533784103546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "log_reg1_evaluator = BinaryClassificationEvaluator(labelCol=\"INJURY_SEVERITY_BINARY\", rawPredictionCol=\"prediction\")\n",
    "log_reg1_auc = log_reg1_evaluator.evaluate(log_reg1_summary.predictions)\n",
    "\n",
    "print(\"Logistic Regression AUC: \", log_reg1_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481bff77",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b631685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# create a ParamGrid for hyperparameter tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(log_reg1.regParam, [0.001, 0.01, 0.1])\n",
    "             .addGrid(log_reg1.elasticNetParam, [0.0, 0.1, 0.2])\n",
    "             .addGrid(log_reg1.maxIter, [10,20,30])\n",
    "             .build())\n",
    "\n",
    "\n",
    "# create 2-fold CrossValidator\n",
    "cv = CrossValidator(estimator=log_reg1,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=log_reg1_evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "# run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "predictions = cvModel.transform(test_data)\n",
    "log_reg1_auc_tuned = log_reg1_evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Tuned Logistic Regression AUC: \", log_reg1_auc_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e86f6",
   "metadata": {},
   "source": [
    "### Precision, Recall and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e7aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regParam: 0.01\n",
      "Best elasticNetParam: 0.0\n",
      "Best maxIter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7629:===================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Precision by Label: [0.8108985814557248, 0.7638728647154829]\n",
      "Logistic Regression Recall by Label: [0.7444837463978956, 0.8264230104228258]\n",
      "Logistic Regression Accuracy: 0.7854576855474465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "best_model = cvModel.bestModel\n",
    "print(\"Best regParam:\", best_model._java_obj.getRegParam())\n",
    "print(\"Best elasticNetParam:\", best_model._java_obj.getElasticNetParam())\n",
    "print(\"Best maxIter:\", best_model._java_obj.getMaxIter())\n",
    "\n",
    "log_reg1_precision = log_reg1_summary.precisionByLabel\n",
    "log_reg1_recall = log_reg1_summary.recallByLabel\n",
    "log_reg_1_accuracy = log_reg1_summary.accuracy\n",
    "\n",
    "\n",
    "print(\"Logistic Regression Precision by Label:\", log_reg1_precision)\n",
    "print(\"Logistic Regression Recall by Label:\", log_reg1_recall)\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_1_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
