{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:13:51.723112Z",
     "start_time": "2024-12-16T07:13:27.797548Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 12:34:17 WARN Utils: Your hostname, Victors-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.68.65 instead (on interface en0)\n",
      "24/12/17 12:34:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/17 12:34:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+---------------+---------------+--------+---------+------------+\n",
      "|           CASE_ID|ACCIDENT_YEAR|PROC_DATE|JURIS|COLLISION_DATE|COLLISION_TIME|OFFICER_ID|REPORTING_DISTRICT|DAY_OF_WEEK|POPULATION|CNTY_CITY_LOC|BEAT_TYPE|CHP_BEAT_TYPE|     PRIMARY_RD|   SECONDARY_RD|DISTANCE|DIRECTION|INTERSECTION|\n",
      "+------------------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+---------------+---------------+--------+---------+------------+\n",
      "|100010101011401155|         2001| 20010416| 0100|      20010101|           114|      1155|                 0|          1|         4|          198|        0|            0|      DUBLIN BL|    SCARLETT CT|     267|        W|           N|\n",
      "|100010103174503131|         2001| 20010416| 0100|      20010103|          1745|      3131|                10|          3|         4|          198|        0|            0|   DOUGHERTY RD|  AMADOR VLY BL|      80|        N|           N|\n",
      "|100010104134002415|         2001| 20010608| 0100|      20010104|          1340|      2415|                 0|          4|         4|          198|        0|            0|   DOUGHERTY RD| RT 580 WBOFF/R|       0|        -|           Y|\n",
      "|100010104170500911|         2001| 20010416| 0100|      20010104|          1705|       911|                 0|          4|         4|          198|        0|            0|   SAN RAMON RD|      DUBLIN BL|      20|        S|           N|\n",
      "|100010104184400911|         2001| 20010416| 0100|      20010104|          1844|       911|                 0|          4|         4|          198|        0|            0|FREDERICKSON LN|FREDERICKSON CT|       0|        -|           Y|\n",
      "+------------------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+---------------+---------------+--------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|CASE_ID|PARTY_NUMBER|VICTIM_ROLE|VICTIM_SEX|VICTIM_AGE|VICTIM_DEGREE_OF_INJURY|VICTIM_SEATING_POSITION|VICTIM_SAFETY_EQUIP1|VICTIM_SAFETY_EQUIP2|VICTIM_EJECTED|\n",
      "+-------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|3495044|           2|          2|         M|        21|                      0|                      3|                   P|                   G|             0|\n",
      "|3495044|           2|          2|         M|        12|                      0|                      2|                   P|                   C|             0|\n",
      "|3507861|           2|          2|         M|        60|                      0|                      9|                   -|                   -|             3|\n",
      "|3511283|           1|          1|         F|        43|                      3|                      1|                   M|                   G|             0|\n",
      "|3511287|           1|          5|         M|        28|                      3|                      1|                   P|                   -|             0|\n",
      "+-------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Collision_Victim_Analysis\")\n",
    "         .config(\"spark.executor.memory\", \"4g\")  # Increase executor memory\n",
    "         .config(\"spark.executor.cores\", \"2\")  # Number of cores per executor\n",
    "         .config(\"spark.driver.memory\", \"4g\")  # Increase driver memory\n",
    "         .getOrCreate())\n",
    "# Load cleaned collision data\n",
    "collision_df = spark.read.csv(\"clean_collision_records.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load cleaned victim data\n",
    "victim_df = spark.read.csv(\"clean_victim_records.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Verify loaded data\n",
    "collision_df.show(5)\n",
    "victim_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4d511e33aa92e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:15:03.039263Z",
     "start_time": "2024-12-16T07:14:03.061881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/17 12:34:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/12/17 12:34:35 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|CASE_ID|ACCIDENT_YEAR|PROC_DATE|JURIS|COLLISION_DATE|COLLISION_TIME|OFFICER_ID|REPORTING_DISTRICT|DAY_OF_WEEK|POPULATION|CNTY_CITY_LOC|BEAT_TYPE|CHP_BEAT_TYPE|PRIMARY_RD|SECONDARY_RD|DISTANCE|DIRECTION|INTERSECTION|PARTY_NUMBER|VICTIM_ROLE|VICTIM_SEX|VICTIM_AGE|VICTIM_DEGREE_OF_INJURY|VICTIM_SEATING_POSITION|VICTIM_SAFETY_EQUIP1|VICTIM_SAFETY_EQUIP2|VICTIM_EJECTED|\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           1|          2|         M|        17|                      0|                      3|                   C|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           1|          2|         M|        17|                      0|                      6|                   C|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           2|          1|         M|        16|                      4|                      1|                   G|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           2|          2|         M|        16|                      4|                      3|                   G|                   -|             0|\n",
      "|    269|         2002| 20030523| 5202|      20020205|          1235|       595|              RBPD|          2|         3|         5202|        0|            0|  UNION ST|  DOUGLAS ST|      10|        S|           N|           1|          2|         M|        17|                      0|                      3|                   C|                   -|             0|\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+------------------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after join: 18710630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Join the datasets on CASE_ID\n",
    "combined_df = collision_df.join(victim_df, \"CASE_ID\", \"inner\")\n",
    "\n",
    "\n",
    "# Show the joined data\n",
    "combined_df.show(5)\n",
    "print(\"Total records after join:\", combined_df.count())\n",
    "\n",
    "# change victim degree of injury to binary values\n",
    "combined_df = combined_df.withColumn(\"INJURY_SEVERITY_BINARY\", when((combined_df[\"VICTIM_DEGREE_OF_INJURY\"] == 1) | (combined_df[\"VICTIM_DEGREE_OF_INJURY\"] == 2), 1).otherwise(0))\n",
    "\n",
    "# drop reporting district column - unused\n",
    "cols_to_drop = ['REPORTING_DISTRICT']\n",
    "combined_df = combined_df.drop(*cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c2053df489a9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:25:12.597643Z",
     "start_time": "2024-12-16T07:22:39.047671Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------+-----+--------------+--------------+----------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+----------------------+-----------------------------+-----------------------------+--------------------------+--------------------+---------------+------------------+---------------------------+---------------------------+------------------------+------------------+-------------+----------------+\n",
      "|CASE_ID|ACCIDENT_YEAR|PROC_DATE|JURIS|COLLISION_DATE|COLLISION_TIME|OFFICER_ID|DAY_OF_WEEK|POPULATION|CNTY_CITY_LOC|BEAT_TYPE|CHP_BEAT_TYPE|PRIMARY_RD|SECONDARY_RD|DISTANCE|DIRECTION|INTERSECTION|PARTY_NUMBER|VICTIM_ROLE|VICTIM_SEX|VICTIM_AGE|VICTIM_DEGREE_OF_INJURY|VICTIM_SEATING_POSITION|VICTIM_SAFETY_EQUIP1|VICTIM_SAFETY_EQUIP2|VICTIM_EJECTED|INJURY_SEVERITY_BINARY|VICTIM_DEGREE_OF_INJURY_index|VICTIM_SEATING_POSITION_index|VICTIM_SAFETY_EQUIP2_index|VICTIM_EJECTED_index|DIRECTION_index|INTERSECTION_index|VICTIM_DEGREE_OF_INJURY_vec|VICTIM_SEATING_POSITION_vec|VICTIM_SAFETY_EQUIP2_vec|VICTIM_EJECTED_vec|DIRECTION_vec|INTERSECTION_vec|\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+----------------------+-----------------------------+-----------------------------+--------------------------+--------------------+---------------+------------------+---------------------------+---------------------------+------------------------+------------------+-------------+----------------+\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |1           |2          |M         |17        |0                      |3                      |C                   |-                   |0             |0                     |1.0                          |1.0                          |1.0                       |0.0                 |1.0            |0.0               |(7,[1],[1.0])              |(18,[1],[1.0])             |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |1           |2          |M         |17        |0                      |6                      |C                   |-                   |0             |0                     |1.0                          |2.0                          |1.0                       |0.0                 |1.0            |0.0               |(7,[1],[1.0])              |(18,[2],[1.0])             |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |2           |1          |M         |16        |4                      |1                      |G                   |-                   |0             |0                     |0.0                          |0.0                          |1.0                       |0.0                 |1.0            |0.0               |(7,[0],[1.0])              |(18,[0],[1.0])             |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |2           |2          |M         |16        |4                      |3                      |G                   |-                   |0             |0                     |0.0                          |1.0                          |1.0                       |0.0                 |1.0            |0.0               |(7,[0],[1.0])              |(18,[1],[1.0])             |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |\n",
      "|269    |2002         |20030523 |5202 |20020205      |1235          |595       |2          |3         |5202         |0        |0            |UNION ST  |DOUGLAS ST  |10      |S        |N           |1           |2          |M         |17        |0                      |3                      |C                   |-                   |0             |0                     |1.0                          |1.0                          |1.0                       |0.0                 |1.0            |0.0               |(7,[1],[1.0])              |(18,[1],[1.0])             |(25,[1],[1.0])          |(10,[0],[1.0])    |(4,[1],[1.0])|(5,[0],[1.0])   |\n",
      "+-------+-------------+---------+-----+--------------+--------------+----------+-----------+----------+-------------+---------+-------------+----------+------------+--------+---------+------------+------------+-----------+----------+----------+-----------------------+-----------------------+--------------------+--------------------+--------------+----------------------+-----------------------------+-----------------------------+--------------------------+--------------------+---------------+------------------+---------------------------+---------------------------+------------------------+------------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Categorical columns to encode\n",
    "encoded_columns = [\"VICTIM_DEGREE_OF_INJURY\",\"VICTIM_SEATING_POSITION\",\"VICTIM_SAFETY_EQUIP2\",\"VICTIM_EJECTED\", \"DIRECTION\", \"INTERSECTION\"]\n",
    "\n",
    "# Index and encode categorical columns\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid='skip') for c in encoded_columns]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_vec\") for c in encoded_columns]\n",
    "\n",
    "# Assemble features into a single column\n",
    "#feature_columns = [\"DAY_OF_WEEK\", \"COLLISION_TIME\", \"VICTIM_AGE\", \"VICTIM_SEX\", \"DIRECTION_vec\"]\n",
    "\n",
    "# Create the assembler\n",
    "#assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Combine all transformations into a Pipeline\n",
    "encoded_pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Transform the data\n",
    "encoded_df = encoded_pipeline.fit(combined_df).transform(combined_df)\n",
    "\n",
    "encoded_df.show(5, truncate=False)\n",
    "\n",
    "\n",
    "# Select final features and target\n",
    "#final_df = prepared_df.select(\"features\", \"VICTIM_DEGREE_OF_INJURY_index\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "#final_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ba13ca6b0879a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T07:21:23.616593Z",
     "start_time": "2024-12-16T07:21:23.613943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Combined DataFrame:\n",
      "['CASE_ID', 'ACCIDENT_YEAR', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'DAY_OF_WEEK', 'POPULATION', 'CNTY_CITY_LOC', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'PARTY_NUMBER', 'VICTIM_ROLE', 'VICTIM_SEX', 'VICTIM_AGE', 'VICTIM_DEGREE_OF_INJURY', 'VICTIM_SEATING_POSITION', 'VICTIM_SAFETY_EQUIP1', 'VICTIM_SAFETY_EQUIP2', 'VICTIM_EJECTED', 'INJURY_SEVERITY_BINARY']\n",
      "Columns in Encoded DataFrame:\n",
      "['CASE_ID', 'ACCIDENT_YEAR', 'PROC_DATE', 'JURIS', 'COLLISION_DATE', 'COLLISION_TIME', 'OFFICER_ID', 'DAY_OF_WEEK', 'POPULATION', 'CNTY_CITY_LOC', 'BEAT_TYPE', 'CHP_BEAT_TYPE', 'PRIMARY_RD', 'SECONDARY_RD', 'DISTANCE', 'DIRECTION', 'INTERSECTION', 'PARTY_NUMBER', 'VICTIM_ROLE', 'VICTIM_SEX', 'VICTIM_AGE', 'VICTIM_DEGREE_OF_INJURY', 'VICTIM_SEATING_POSITION', 'VICTIM_SAFETY_EQUIP1', 'VICTIM_SAFETY_EQUIP2', 'VICTIM_EJECTED', 'INJURY_SEVERITY_BINARY', 'VICTIM_DEGREE_OF_INJURY_index', 'VICTIM_SEATING_POSITION_index', 'VICTIM_SAFETY_EQUIP2_index', 'VICTIM_EJECTED_index', 'DIRECTION_index', 'INTERSECTION_index', 'VICTIM_DEGREE_OF_INJURY_vec', 'VICTIM_SEATING_POSITION_vec', 'VICTIM_SAFETY_EQUIP2_vec', 'VICTIM_EJECTED_vec', 'DIRECTION_vec', 'INTERSECTION_vec']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in Encoded DataFrame: 18710617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in Encoded DataFrame: 18710613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Columns in Combined DataFrame:\")\n",
    "print(combined_df.columns)\n",
    "\n",
    "print(\"Columns in Encoded DataFrame:\")\n",
    "print(encoded_df.columns)\n",
    "\n",
    "print(\"number of records in Encoded DataFrame:\", encoded_df.count())\n",
    "encoded_df = encoded_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "print(\"number of records in Encoded DataFrame:\", encoded_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6ba41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 120:====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for Class 1 (Minority): 15.999142350933239\n",
      "Weight for Class 0 (Majority): 0.5161299247622528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight for Class 0 (Majority):\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight_for_class_0)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Add the class weights to the DataFrame\u001b[39;00m\n\u001b[1;32m     17\u001b[0m log_output \u001b[38;5;241m=\u001b[39m log_output\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 19\u001b[0m     when(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINJURY_SEVERITY_BINARY\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[43mlit\u001b[49m(weight_for_class_1))\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39motherwise(lit(weight_for_class_0))\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m log_reg1_data \u001b[38;5;241m=\u001b[39m log_output\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINJURY_SEVERITY_BINARY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m log_reg1_data\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lit' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Create a Logistic Regression model to predict injury severity (1 is fatal/severe, 0 is non-fatal/non-severe), and assemble the features into a single column\n",
    "logistic_assembler = VectorAssembler(inputCols=[\"VICTIM_SEATING_POSITION_vec\", \"VICTIM_SAFETY_EQUIP2_vec\", \"VICTIM_EJECTED_vec\"], outputCol=\"features\")\n",
    "log_output = logistic_assembler.transform(encoded_df)\n",
    "total_count = log_output.count()\n",
    "\n",
    "# Calculate class weights\n",
    "class_1_count = log_output.filter(col(\"INJURY_SEVERITY_BINARY\") == 1).count()\n",
    "class_0_count = log_output.filter(col(\"INJURY_SEVERITY_BINARY\") == 0).count()\n",
    "weight_for_class_1 = total_count / (2 * class_1_count)\n",
    "weight_for_class_0 = total_count / (2 * class_0_count)\n",
    "\n",
    "print(\"Weight for Class 1 (Minority):\", weight_for_class_1)\n",
    "print(\"Weight for Class 0 (Majority):\", weight_for_class_0)\n",
    "\n",
    "# Add the class weights to the DataFrame\n",
    "log_output = log_output.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"INJURY_SEVERITY_BINARY\") == 1, lit(weight_for_class_1))\n",
    "    .otherwise(lit(weight_for_class_0))\n",
    ")\n",
    "\n",
    "log_reg1_data = log_output.select(\"features\", \"INJURY_SEVERITY_BINARY\", \"weight\")\n",
    "log_reg1_data.show(5)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = log_reg1_data.randomSplit([0.7, 0.3], seed=7122)\n",
    "\n",
    "\n",
    "\n",
    "# fit the model\n",
    "log_reg1 = LogisticRegression(labelCol=\"INJURY_SEVERITY_BINARY\", featuresCol=\"features\", weightCol=\"weight\")\n",
    "log_reg1_model = log_reg1.fit(train_data)\n",
    "\n",
    "# print summaries\n",
    "log_reg1_summary = log_reg1_model.summary\n",
    "log_reg1_summary.predictions.show(5)\n",
    "log_reg1_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b71d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8056:===================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC:  0.503480519126268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "log_reg1_evaluator = BinaryClassificationEvaluator(labelCol=\"INJURY_SEVERITY_BINARY\", rawPredictionCol=\"prediction\")\n",
    "log_reg1_auc = log_reg1_evaluator.evaluate(log_reg1_summary.predictions)\n",
    "\n",
    "print(\"Logistic Regression AUC: \", log_reg1_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b067f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# create a ParamGrid for hyperparameter tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(log_reg1.regParam, [0.001 ,0.01, 0.1])\n",
    "             .addGrid(log_reg1.elasticNetParam, [0.0, 0.1, 0.2])\n",
    "             .addGrid(log_reg1.maxIter, [3,5,7])\n",
    "             .build())\n",
    "\n",
    "\n",
    "# create 2-fold CrossValidator\n",
    "cv = CrossValidator(estimator=log_reg1,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=log_reg1_evaluator,\n",
    "                    numFolds=2)\n",
    "\n",
    "# run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "predictions = cvModel.transform(test_data)\n",
    "log_reg1_auc_tuned = log_reg1_evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Tuned Logistic Regression AUC: \", log_reg1_auc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20056323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab18086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
